
# 事务
> 事务(Transaction)是访问和更新数据库的程序执行单元:事务中可能包含一个或多个sql语句，这些语句要么都执行，要么都不执行

MySQL中服务器层不管理事务,事务是由存储引擎实现的

典型的MySQL事务是如下操作的：
```
start transaction;
sql；
sql;
sql;
commit;
```
如果sql语句执行出现问题，会调用rollback，回滚所有已经执行成功的sql语句。当然，也可以在事务中直接使用rollback语句进行回滚

## 数据库事务的四个特性 ACID
> 按照严格的标准，只有同时满足ACID特性才是事务；但是在各大数据库厂商的实现中，真正满足ACID的事务少之又少

* Atomicity 原子性整个事务中的所有操作,要么全部完成,要么全部不完成,不可能停滞在中间某个环节.事务在执行过程中发生错误.会被Rollback到事务开始前的状态，就像这个事务从来没有执行过一样
* Consistency：在事务开始之前和事务结束以后,数据库中的数据的状态要确保一致
* Isolation:一个事务的执行不能其它事务干扰。即一个事务内部的操作及使用的数据对其它并发事务是隔离的,并发执行的各个事务之间不能互相干扰
* Durability持久性：在事务完成以后,该事务所对数据库所作的更改便持久的保存在数据库之中,并不会被回滚

## 并发事务处理带来的问题
* 更新丢失(Lost Update):事务A和事务B选择同一行，然后基于最初选定的值更新该行时，由于两个事务都不知道彼此的存在，就会发生丢失更新问题
* 脏读(Dirty Read):一个事务读到了另一个未提交事务修改过的数据
* 不可重复读(Non-Repeatable Read):事务A多次读取同一数据,事务B在事务A多次读取的过程中,对数据作了更新并提交，导致事务A多次读取同一数据时,结果不一致
* 幻读(Phantom):一个事务先根据某些条件查询出一些记录，之后另一个事务又向表中插入了符合这些条件的记录，原先的事务再次按照该条件查询时，能把另一个事务插入的记录也读出来

幻读和不可重复读的区别：
* 不可重复读的重点是修改：在同一事务中，同样的条件，第一次读的数据和第二次读的数据不一样
* 幻读的重点在于新增或者删除：在同一事务中，同样的条件,，第一次和第二次读出来的记录数不一样

并发事务处理带来的问题的解决办法：
* 需要应用程序对要更新的数据加必要的锁来解决，因此，防止更新丢失应该是应用的责任
* 脏读”,“不可重复读”和“幻读”,其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决
 * 一种是加锁:在读取数据前，对其加锁，阻止其他事务对数据进行修改。
 * 另一种是数据多版本并发控制（MultiVersion Concurrency Control，简称 MVCC 或 MCC)：不用加任何锁,通过一定机制生成一个数据请求时间点的一致性数据快照(Snapshot),并用这个快照来提供一定级别 (语句级或事务级)的一致性读取。从用户的角度来看，好象是数据库可以提供同一数据的多个版本

## 事务隔离级别
> 事务隔离实质上就是使事务在一定程度上“串行化”进行，这显然与“并发”是矛盾的
```
show variables like 'tx_isolation'
```
* READ-UNCOMMITTED(读未提交)：最低的隔离级别，事务A可以读取到事务B修改过但未提交的数据，可能会导致脏读、幻读或不可重复读。
* READ-COMMITTED(读已提交)：事务B只能在事务A修改过并且已提交后才能读取到事务B修改的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。
* REPEATABLE-READ(可重复读)：就是在开始读取数据(事务开启)时，不再允许修改操作，可以阻止脏读和不可重复读，但幻读仍有可能发生。
* SERIALIZABLE(可串行化)：最高的隔离级别，完全服从ACID的隔离级别。会在读取的每一行数据上都加锁,所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰

MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）
* InnoDB 存储引擎在 REPEATABLE-READ（可重读）事务隔离级别下使用的是Next-Key Lock算法，因此可以避免幻读的产生，这与其他数据库系统(如 SQL Server)是不同的

---
# 事务的实现
> 事务的实现就是如何实现ACID特性。
* 事务的原子性是通过 undo log 来实现的
* 事务的持久性性是通过 redo log 来实现的
* 事务的隔离性是通过 (读写锁+MVCC)来实现的
* 而事务的一致性是通过原子性，持久性，隔离性来实现的

## 事务日志
> 记录着数据库运行期间各种状态信息

<img width="418" alt="Screen Shot 2021-11-13 at 7 55 43 PM" src="https://user-images.githubusercontent.com/27160394/141642835-14e7f9a1-7161-49cc-8459-180a864b3b64.png">


> 脏页: 当内存数据页跟磁盘数据页内容不一致的时候
> 干净页: 内存数据写入磁盘后.内存和磁盘上的数据页内容就一致
> 刷脏页的过程（flush）: MySQL从内存更新到磁盘的过程

InnoDB 刷脏页的时机
* 内存中的redo log 写满了，这时系统就会停止所有更新操作，把checkoutpoint往前推,往前推进之后，就要把两个点之间的日志对应的所有脏页都 flush 到磁盘上
* 系统中内存不足时，当这个时候需要新的数据页到内存中，就要淘汰掉一些数据页，如果淘汰的是“脏页”，就要先将“脏页”写到磁盘
* 数据库系统空闲时,当然平时忙的时候也会尽量刷脏页.
* 数据库正常关闭, 此时需要将所有脏页刷新到磁盘.


MySQL 中有一个机制，刷脏页的时候如果数据页旁边的数据页也是脏页，那么就会一起刷掉，而且这个逻辑是可以蔓延的，所以对于每个相邻的数据页，都会被一起刷(innodb_flush_neighbors)

Write-Ahead Log (WAL)
* 所有的修改在提交之前都要先写入log文件中,对数据库中的数据就行了修改，必须保证日志先于数据落盘
* 如果数据库在日志落盘前crash，那么相应的数据修改会回滚。
* 在日志落盘后crash，会保证相应的修改不丢失
* 用了这项技术，除了修改的数据，还需要多写一份日志，也就是磁盘写入量反而增大，
* 用日志把随机IO变成顺序IO 
  * 由于日志是顺序的且往往先存在内存里然后批量往磁盘刷新，相比数据的离散写入，日志的写入开销比较小,InnoDB
  * InnoDB 使用一个后台线程智能地刷新这些变更到数据文件。这个线程可以批量组合写入，使得数据写入更顺序，以提高效率
* 一旦日志安全写到磁盘，事务就持久化了，即使断电了，InnoDB可以重放日志并且恢复已经提交的事务


InnoDB对于数据文件和日志文件的刷盘遵守WAL(Write ahead redo log) 和Force-log-at-commit两种规则
* WAL要求数据的变更写入到磁盘前，首先必须将内存中的日志写入到磁盘
* Force-log-at-commit要求当一个事务提交时，所有产生的日志都必须刷新到磁盘上(事务中的操作,都会先写入存储引擎的日志缓冲中)

事务日志可以帮助提高事务的效率
* 使用事务日志，存储引擎在修改表的数据时只需要修改其内存拷贝，再把该修改行为记录到持久在硬盘上的事务日志中，而不用每次都将修改的数据本身持久到磁盘
* 事务日志持久以后，内存中被修改的数据在后台可以慢慢刷回到磁盘
* 如果数据的修改已经记录到事务日志并持久化，但数据本身没有写回到磁盘，此时系统崩溃，存储引擎在重启时能够自动恢复这一部分修改的数据
* 事务日志采用的是追加的方式，因此写日志的操作是磁盘上一小块区域内的顺序I/O，而不像随机I/O需要在磁盘的多个地方移动磁头，所以采用事务日志的方式相对来说要快得多。

主要日志
1. server 层 binLog：在对数据进行增删改之后，都将会记录一条 binlog,可用于数据归档和备份
2. InnoDB存储引擎层的日志: 重做日志redo和回滚日志undo
   * redo log 在对数据进行增删改之后，都将会记录一条 redolog 通常是物理日志，记录的是数据页的物理修改，而不是某一行或某几行修改成怎样怎样
     * 崩溃后恢复事务数据
     * 减少更新数据时的磁盘 IO 访问
   * undo log 在事务中对数据每进行一次修改便会记录一次 undolog 用于将最新数据恢复到之前事务版本。undo log一般是逻辑日志，根据每行记录进行记录

binlog 和 redolog 存在一个共同的数据字段 XID，通过这个字段可以将 redolog 和 binlog 关联起来，可用于事务恢复


## redo log
> 用于保证事务持久性
* redo log是innodb层产生的
* 用来记录事务操作引起数据的变化，记录的是数据页的物理修改
* 二进制日志先于redo log被记录

该日志文件由两部分组成:
* 重做日志缓冲（redo log buffer)，在 log buffer里
* 重做日志文件（redo log file) ,后者在磁盘中
* innodb存储引擎存储数据的单元是页(和SQL Server中一样)，所以redo log也是基于页的格式来记录的


WAL的机制在redo log的使用场景
*  Innodb引擎会把数据先插入redo log(也是写入磁盘，顺序写入，比较快)当中，并更新内存(db buffer)，这个时候更新就算完成了
*  此时，内存(db buffer)中的数据和磁盘数据(data file)对应的数据不同，我们认为内存中的数据是脏数据
*  db buffer再选择合适的时机将数据持久化到data file中。
* 这种顺序可以保证在需要故障恢复时恢复最后的修改操作


Redo log 过程
* 事务开启时，事务中的操作，都会先写入存储引擎的日志缓冲(redo log buffer)中
* 在事务提交之前，这些缓冲的日志都需要提前刷新到磁盘上持久化(WAL,Force-log-at-commit)
* 当事务提交之后，在Buffer Pool中映射的数据文件根据redo log的记录刷新到磁盘
* 此时如果数据库崩溃或者宕机，那么当系统重启进行恢复时，就可以根据 redo log 中记录的日志，把数据库恢复到崩溃前的一个状态
* 未完成的事务，可以继续提交，也可以选择回滚，这基于恢复的策略而定

Redo log 持久化策略 
> 为了确保每次记录都能够写入到磁盘中的日志中，每次将redo log buffer中的日志写入redo log file的过程中都会调用一次操作系统的fsync操作(用于同步内存中所有已修改的文件数据到储存设备)
* MySQL的 log buffer 处于用户空间的内存中。要写入到磁盘上的log file 中 ，中间还要经过操作系统内核空间的os buffer
* 调用`fsync()`的作用就是将`OS buffer`中的日志刷到磁盘上的`log file`中。

<img width="347" alt="Screen Shot 2021-11-13 at 7 40 40 PM" src="https://user-images.githubusercontent.com/27160394/141642394-cc5a1287-726c-4310-8dee-0ac1d05cfa55.png">

日志刷盘时机 (innodb_flush_log_at_trx_commit)
* 0：在事务提交时，innodb 不会立即触发将缓存日志写到磁盘文件的操作，而是每秒触发一次缓存日志回写磁盘操作，并调用系统函数 fsync 刷新 IO 缓存。这种方式效率最高，也最不安全
* 1：在每个事务提交时，innodb 立即将缓存中的 redo 日志回写到日志文件，并调用 fsync 刷新 IO 缓存。
* 2：在每个事务提交时，innodb 立即将缓存中的 redo 日志回写到日志文件，但并不马上调用 fsync 来刷新 IO 缓存，而是每秒只做一次磁盘IO 缓存刷新操作。只要操作系统不发生崩溃，数据就不会丢失，这种方式是对性能和数据安全的折中，其性能和数据安全性介于其他两种方式之间。

Redo Log LSN
> 日志的逻辑序列号(log sequence number)
* lsn占用 8 个字节。LSN 的值会随着日志的写入而逐渐增大。
* 可以简单理解LSN就是记录从开始到现在已经产生了多少字节的Redo log值

<img width="509" alt="Screen Shot 2021-11-13 at 9 24 13 PM" src="https://user-images.githubusercontent.com/27160394/141645439-7064d0be-a2f5-4d67-8a72-5630ee5ef212.png">


### Redo log 两阶段提交

1. UPDATE语句的结果写入内存，同时将这个操作写入redo log，此时redo log处于prepare状态，并告知执行器随时可以提交事物
2. 执行器生成这个操作的binlog，并写入binlog日志. 
3. 执行器通知将之前处于prepare状态改为commit状态，更新完成。

保证了redo log和binlog的一致性
* 先写redo log后写binlog,redo log会恢复crash的语句，但是如果用这产生时的binlog去恢复数据库就会丢失这条记录，此时两个日志恢复的数据库数据就产生了差异
* 先写binlog后写redo log,redo log中还没写,此时异常重启后这个事务是无效的,所以无法恢复,但是binlog中有这条数据,当用此时的binlog文件去恢复数据库的时候,就会比当前的数据库数据多一条记录。


Redo log 容灾恢复过程
* 判断 redo log 是否完整，如果判断是完整（commit）的，直接用 Redo log 恢复
* 如果 redo log 只是预提交 prepare 但不是 commit 状态，这个时候就会去判断 binlog 是否完整，如果完整就提交 Redo log，用 Redo log 恢复，不完整就回滚事务，丢弃数据



### redo log 作用
* 由于Redo Log在持久化过程中顺序写文件的特性，使得持久化Redo Log的代价要远远小于持久化数据页，因此通常情况下，数据页的持久化要远落后于Redo Log
* 确保事务的持久性: 防止在发生故障的时间点，尚有脏页未写入磁盘，redo log来记录已成功提交事务的修改信息，并且会把redo log持久化到磁盘，系统重启之后在读取redo log恢复最新数据


```
crash-safe 即在 InnoDB 存储引擎中，事务提交过程中任何阶段，MySQL突然奔溃，重启后都能保证事务的完整性，已提交的数据不会丢失，未提交完整的数据会自动进行回滚。这个能力依赖的就是redo log和unod log两个日志
```

> Mysql怎么保证持久性的？

利用Innodb的redo log
* 当做数据修改的时候，不仅在内存中操作，还会在redo log中记录这次操作
* 当事务提交的时候，会将redo log日志进行刷盘
* 当数据库宕机重启的时候，会将redo log中的内容恢复到数据库中
* 再根据undo log和binlog内容决定回滚数据还是提交数据。

* 物理的日志可看作是实际数据库中数据页上的变化信息，只看重结果，而不在乎是通过“何种途径”导致了这种结果
* 逻辑的日志可看作是通过了某一种方法或者操作手段导致数据发生了变化，存储的是逻辑性的操作

## ChangeBuffer
> ChangeBuffer 主要节省的则是随机读磁盘的 IO 消耗

<img width="457" alt="Screen Shot 2021-11-13 at 8 11 35 PM" src="https://user-images.githubusercontent.com/27160394/141643308-d826c8f9-2cbb-4fba-a266-3cafa0742252.png">

ChangeBuffer 与 Redo log 区别
* Redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写）ChangeBuffer 主要节省的则是随机读磁盘的 IO 消耗
* ChangeBuffer机制不是一直会被应用到，仅当待操作的数据页当前不在内存中，需要先读磁盘加载数据页时，ChangeBuffer 才有用武之地

merge
* 从磁盘读入数据页到内存（老版本的数据页）；
* 从change buffer 里找出这个数据页的 change buffer 记录(可能有多个),依次应用，得到新版数据页；
* 写redo log。这个 redo log 包含了数据的变更和 change buffer 的变更。

## undo log
> 用于记录数据被修改前的信息,保存了事务发生之前的数据的一个版本，可以用于回滚

在事务执行的过程中
* 除了记录redo log，还会记录一定量的undo log。
* undo log记录了数据在每个操作前的状态，用于记录数据被修改前的信息
* 如果事务执行过程中需要回滚，就可以根据undo log进行回滚操作
* undo log不redo log不一样，它属于逻辑日志。它对SQL语句执行相关的信息进行记录
* Undo记录的是已部分完成并且写入硬盘的未完成的事务


### undo log的存储方式
* innodb存储引擎对undo的管理采用段的方式。rollback segment称为回滚段，每个回滚段中有1024个undo log segment
* undo log是采用段(segment)的方式来记录的，每个undo操作在记录的时候占用一个undo log segment

### undo log 有什么作用
* 用于保障未提交事务的原子性:记录事务修改之前版本的数据信息，因此假如由于系统错误或者rollback操作而回滚的话可以根据undo log的信息将数据从逻辑上恢复至事务之前的状态
* 多个行版本控制(MVCC): 当读取的某一行被其他事务锁定时，它可以从undo log中分析出该行记录以前的数据是什么，从而提供该行版本信息


> Mysql怎么保证原子性的？
是利用Innodb的undo log。
* undo log属于逻辑日志，它记录的是sql执行相关的信息
* 当事务对数据库进行修改时，InnoDB会生成对应的undo log
* undo log主要记录的是数据的逻辑变化，为了在发生错误时回滚之前的操作
* 如果事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。


不同
* redo_log是恢复提交事务修改的页操作，而undo_log是回滚行记录到特定版本。
* 二者记录的内容也不同，redo_log是物理日志，记录页的物理修改操作，而undo_log是逻辑日志，根据每行记录进行记录


## binlog(二进制日志)
> 事务提交的时候，一次性将事务中的sql语句（一个事物可能对应多个sql语句）按照一定的格式记录到binlog中


* 二进制日志binlog是服务层的日志
* binlog主要记录数据库的变化情况，内容包括数据库所有的更新操作
* 因此有了binlog可以很方便的对数据进行复制和备份
* binlog是基于point-in-time recovery，保证服务器可以基于时间点对数据进行恢复，或者对数据进行备份


Binlog 记录过程及刷盘时机
* 先写Binlog Buffer，
* 然后通过刷盘时机，控制刷入OS Buffer，控制fsync()进行写入Binlog File日记磁盘的过程
  * MySQL是通过参数sync_binlog参数来控制刷盘时机
  * 0表示由系统自行判断何时调用sync()写入磁盘
  * 1表示每次事务commit都要调用fsync()写入磁盘
  * N表示每N个事务，才会调用fsync()写入磁盘。


> redo/undo log 和 binlog
* 层次不同. redo/undo 是 innodb 引擎层维护的，而 binlog 是 mysql server 层维护的，跟采用何种引擎没有关系，记录的是所有引擎的更新操作的日志记录
* 记录内容不同。redo/undo 记录的是 每个页/每个数据 的修改情况，属于物理日志+逻辑日志结合的方式（redo log 是物理日志，undo log 是逻辑日志）。binlog 记录的都是事务操作内容，binlog 有三种模式：Statement（基于 SQL 语句的复制）、Row（基于行的复制） 以及 Mixed（混合模式）。不管采用的是什么模式，当然格式是二进制的，
* 记录时机不同。redo/undo 在 事务执行过程中会不断的写入，而 binlog 是在 事务最终提交前写入的。binlog 什么时候刷新到磁盘跟参数 sync_binlog 相关
* binlog 事务提交的时候，一次性将事务中的sql语句（一个事物可能对应多个sql语句）
* 文件空间： redo log日志文件固定的，循环写，binLog日志空间不一定，写完可以切换到下一个，追加写



binLog的格式 
>binlog 共有三种存储格式： row, statement 和 混合模式。
1. row： 基于行的复制，不记录每条SQL信息，仅仅记录哪行记录被修改
2. statement: 记录修改数据的SQLm不记录修改的数据, 记录数据的过程，日志量比较小，IO 读写少。缺点是 有些函数并不支持，为了保证能够一致性，必须还需要有一些上下文的信息。
3. 混合模式:  是以上两种模式的混合使用，MySQL 会根据执行的每一条具体的 sql 语句来区分对待记录的日志形式。
 一般的语句修改使用 statment 格式保存 binlog ，如一些函数，statement 无法完成主从复制的操作，则采用 row 格式保存 binlog。


**Binlog日志有以下两个最重要的使用场景**
```
1. 主从复制：在主库中开启Binlog功能，这样主库就可以把Binlog传递给从库，从库拿到Binlog后实现数据恢复达到主从数据一致性。
2. 数据恢复：用于数据库的基于时间点的还原,通过mysqlbinlog工具来恢复数据。

```

### 主从复制
* 从原来的数据库复制一个完全一样的数据库，原来的数据库称作主数据库，复制的数据库称为从数据库
* 从数据库会与主数据库进行数据同步，保持二者的数据一致性
* 主从复制的原理实际上就是通过bin log日志实现的
* bin log日志中保存了数据库中所有SQL语句，通过对bin log日志中SQL的复制，然后再进行语句的执行即可实现从数据库与主数据库的同步

主从复制的过程主要是靠三个线程进行的
1. 一个运行在主服务器中的发送线程，用于发送binlog日志到从服务器
2. I/O线程用于读取主服务器发送过来的binlog日志内容,并拷贝到本地的中继日志中
3. SQL线程用于读取中继日志中关于数据更新的SQL语句并执行，从而实现主从库的数据一致

主从复制的场景
* 通过复制实现数据的异地备份，当主数据库故障时，可切换从数据库，避免数据丢失。
* 可实现架构的扩展，当业务量越来越大，I/O访问频率过高时，采用多库的存储，可以降低磁盘I/O访问的频率，提高单个机器的I/O性能
* 可实现读写分离，使数据库能支持更大的并发
* 实现服务器的负载均衡，通过在主服务器和从服务器之间切分处理客户查询的负荷

> 为什么会有两份日志呢？(redo log and bin log)
* 那么针对只有 redo log日志，没有 binlog 日志，这也是不行的，因为 redo log 是 innodb 持有的，且日志上的记录落盘后会被抹掉。
* 因此需要 binlog 和 redo log 两者同时记录，才能保证当数据库发生宕机重启时，数据不会丢失

> 为啥 Binlog 没有 crash-safe 功能
* redo log 和 binlog 一个是循环写，一个是追加写。
* 也就是说 redo log 只会记录未刷盘的日志，已经刷入磁盘的数据都会从 redo log 这个有限大小的日志文件里删除。
* binlog 是追加日志，保存的是全量日志, binlog 拥有全量的日志，但没有一个标志让 innodb 判断哪些数据已经刷盘
*  binlog 数据库无法判断这两条记录哪条已经写入磁盘，哪条没有写入磁盘
* 但 redo log 不一样，只要刷入磁盘的数据，都会从 redo log 中抹掉，数据库重启后，直接把 redo log 中的数据都恢复至内存就可以了


> 你知道MySQL 有多少种日志吗？
* 错误日志：记录出错信息，也记录一些警告信息或者正确的信息。
* 查询日志：记录所有对数据库请求的信息，不论这些请求是否得到了正确的执行。
* 慢查询日志：设置一个阈值，将运行时间超过该值的所有SQL语句都记录到慢查询的日志文件中。
* 二进制日志：记录对数据库执行更改的所有操作。
* 中继日志：中继日志也是二进制日志，用来给slave 
* 库恢复事务日志：重做日志redo和回滚日志undo


> Mysql怎么保证一致性的？

从数据库层面，数据库通过原子性、隔离性、持久性来保证一致性。
* 也就是说ACID四大特性之中，C(一致性)是目的，A(原子性)、I(隔离性)、D(持久性)是手段，是为了保证一致性
* 数据库必须要实现AID三大特性，才有可能实现一致性

如果你在事务里故意写出违反约束的代码，一致性还是无法保证的
* 因此，还必须从应用层角度考虑。通过代码判断数据库数据是否有效，然后决定回滚还是提交数据

---

# MySQL锁机制
> Mysql怎么保证隔离性的？
利用的是锁和MVCC机制
* (一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性
* (一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性

## 锁
> 数据库锁定机制是数据库为了保证数据的隔离性，而使各种共享资源在被并发访问变得有序所设计的一种规则

理论上每次只锁定当前操作的数据的方案会得到最大的并发度，但是管理锁是很耗资源的事情（涉及获取，检查，释放锁等动作），因此数据库系统需要在高并发响应和系统性能两方面进行平衡


从对数据操作的类型分类：
* 读锁（共享锁）：其他事务可以读，但不能写
* 写锁（排他锁）：：其他事务不能读取，也不能写

从对数据操作的粒度分类：
* 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低（MyISAM 和 MEMORY 存储引擎采用的是表级锁
* 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高（InnoDB 存储引擎既支持行级锁也支持表级锁，但默认情况下是采用行级锁）；
* 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般

## MyISAM表锁
* 表共享读锁 （Table Read Lock）：不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求；
* 表独占写锁 （Table Write Lock）：会阻塞其他用户对同一表的读和写操作；

默认情况下，写锁比读锁具有更高的优先级：当一个锁释放时，这个锁会优先给写锁队列中等候的获取锁请求，然后再给读锁队列中等候的获取锁请求
* 大量的更新操作会造成查询操作很难获得读锁，从而可能永远阻塞
* 同时，一些需要长时间运行的查询操作，也会使写线程“饿死” ，应用中应尽量避免出现长时间运行的查询操

MyISAM加表锁方法：
* MyISAM 在执行查询语句（SELECT）前，会自动给涉及的表加读锁，
* 在执行更新操作（UPDATE、DELETE、INSERT 等）前，会自动给涉及的表加写锁
* 在自动加锁的情况下，MyISAM 总是一次获得 SQL 语句所需要的全部锁，这也正是 MyISAM 表不会出现死锁（Deadlock Free）的原因

可以通过检查`table_locks_waited`和`table_locks_immediate`状态变量来分析系统上的表锁的争夺，如果`Table_locks_waited`的值比较高，则说明存在着较严重的表级锁争用情况

## InnoDB 行锁
InnoDB 实现了以下两种类型的行锁
* 共享锁（S）：多个事务对于同一数据可以共享一把锁m都能访问到数据,但是只能读不能修改.阻止其他事务获得相同数据集的排他锁。
* 排他锁（X）：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁

为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB 还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁
* 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的 IS 锁。
* 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的 IX 锁


### InnoDB加锁方法：
```
//意向锁是 InnoDB 自动加的， 不需用户干预。
select //不会上锁
insert、update、delete //上写锁
```
* 事务可以通过以下语句显式给记录集加共享锁或排他锁
 * 共享锁(S):SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE。 其他 session 仍然可以查询记录，并也可以对该记录加 share mode 的共享锁。但是如果当前事务需要对该记录进行更新操作，则很有可能造成死锁
 * 排他锁(X)：SELECT * FROM table_name WHERE ... FOR UPDATE。其他 session 可以查询该记录，但是不能对该记录加共享锁或排他锁，而是等待获得锁 


解锁（手动）
1. 提交事务（commit）
2. 回滚事务（rollback）
3. kill 阻塞进程

## 加锁机制
乐观锁与悲观锁是两种并发控制的思想，可用于解决丢失更新问题

* 乐观锁会“乐观地”假定大概率不会发生并发更新冲突，访问、处理数据过程中不加锁，只在更新数据时再根据版本号或时间戳判断是否有冲突，有则处理，无则提交事务.
  * 用数据版本（Version）记录机制实现
  * 业务代码
  * 并发量小


* 悲观锁会“悲观地”假定大概率会发生并发更新冲突，访问、处理数据前就加排他锁，在整个数据处理过程中锁定数据，事务提交或回滚后才释放锁
 * 悲观锁是由数据库自己实现了的，要用的时候，我们直接调用数据库的相关语句就可以了
 * 表锁、行锁等
 * 并发量大
 

 
锁模式(InnoDB有三种行锁的算法)
> InnoDB 行锁是通过给索引上的索引项加锁来实现的,只有通过索引条件检索数据，InnoDB 才使用行级锁，否则，InnoDB 将使用表锁！
* 记录锁(Record Locks)：单个行记录上的锁。对索引项加锁，锁定符合条件的行。其他事务不能修改和删除加锁项
* 间隙锁(Gap Locks):当我们使用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁。对于键值在条件范围内但并不存在的记录，叫做“间隙”
 * 对索引项之间的“间隙”加锁，锁定记录的范围,使用间隙锁锁住的是一个区间，而不仅仅是这个区间中的每一条数据
 * GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况
* 临键锁(Next-key Locks)：是记录锁与间隙锁的组合,它的封锁范围既包含索引记录又包含索引区间.(临键锁的主要目的，也是为了避免幻读(Phantom Read).如果把事务的隔离级别降级为RC，临键锁则也会失效。)

> select for update有什么含义，会锁表还是锁行还是其他
* for update 仅适用于InnoDB，且必须在事务块(BEGIN/COMMIT)中才能生效。
* 在进行事务操作时，通过“for update”语句，MySQL会对查询结果集中每行数据都添加排他锁，其他线程对该记录的更新与删除操作都会阻塞。排他锁包含行锁、表锁


InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！


## 死锁
> 指两个或者多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环的现象

死锁产生
* 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环当
* 事务试图以不同的顺序锁定资源时，就可能产生死锁。多个事务同时锁定同一个资源时也可能会产生死锁。
* 锁的行为和顺序和存储引擎相关，以同样的顺序执行语句，有些存储引擎会产生死锁有些不会——死锁有双重原因：真正的数据冲突；存储引擎的实现方式。

检测死锁：数据库系统实现了各种死锁检测和死锁超时的机制。InnoDB存储引擎能检测到死锁的循环依赖并立即返回一个错误

死锁恢复：死锁发生以后，只有部分或完全回滚其中一个事务，才能打破死锁，
* InnoDB目前处理死锁的方法是，将持有最少行级排他锁的事务进行回滚。所以事务型应用程序在设计时必须考虑如何处理死锁，多数情况下只需要重新执行因死锁回滚的事务即可

外部锁的死锁检测
* 发生死锁后，InnoDB 一般都能自动检测到，并使一个事务释放锁并回退，另一个事务获得锁，继续完成事务。
* 但在涉及外部锁，或涉及表锁的情况下，InnoDB 并不能完全自动检测到死锁,这需要通过设置锁等待超时参数 innodb_lock_wait_timeout 来解决
* 人为解决，kill阻塞进程（show processlist）
*  查看死锁：show engine innodb status \G



MyISAM避免死锁：
* 在自动加锁的情况下，MyISAM 总是一次获得 SQL 语句所需要的全部锁，所以 MyISAM 表不会出现死锁。

InnoDB避免死锁：
* 为了在单个InnoDB表上执行多个并发写入操作时避免死锁，可以在事务开始时通过为预期要修改的每个元祖（行）使用SELECT ... FOR UPDATE语句来获取必要的锁
* 在事务中,如果要更新记录,应该直接申请足够级别的锁,即排他锁,而不应先申请共享锁.更新时再申请排他锁,因为这时候当用户再申请排他锁时,其他事务可能又已经获得了相同记录的共享锁,从而造成锁冲突,甚至死锁
* 如果事务需要修改或锁定多个表，则应在每个事务中以相同的顺序使用加锁语句。在应用中，如果不同的程序会并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会
* 改变事务隔离级别
* 可以用 show engine innodb status;命令来确定最后一个死锁产生的原因
---

# MVCC 多版本并发控制
> 通过数据多版本来做到读写分

### MVCC解决什么问题？
* 通过 MVCC 可以让读写互相不阻塞，即读不阻塞写，写不阻塞读，这样就可以提升事务并发处理能力
* 因为 InnoDB 的 MVCC 采用了乐观锁的方式，读取数据时并不需要加锁，对于写操作，也只锁定必要的行
* 一致性读也被称为快照读，当我们查询数据库在某个时间点的快照时，只能看到这个时间点之前事务提交更新的结果，而不能看到这个时间点之后事务提交的更新结果

典型的MVCC实现方式
* 乐观（optimistic）并发控
* 悲观（pressimistic）并发控制

MVCC在mysql中的实现依赖的是undo log与read view
* undo log :undo log 中记录某行数据的多个版本的数据。
* read view :用来判断当前版本数据的可见性

InnoDB 的 MVCC
* 是通过在每行记录后面保存两个隐藏的列来实现一个保存行的创建时间 一个保存行的过期时间（删除时间）存储的并不是真实的时间，而是系统版本号(system version number)
* 每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较

### REPEATABLE READ隔离级别下MVCC如何工作
* SELECT：InnoDB会根据以下两个条件检查每行记录：
  * InnoDB只查找版本早于当前事务版本的数据行，这样可以确保事务读取的行，要么是在开始事务之前已经存在要么是事务自身插入或者修改过的
  * 行的删除版本号要么未定义，要么大于当前事务版本号，这样可以确保事务读取到的行在事务开始之前未被删除
  * 只有符合上述两个条件的才会被查询出来
  
* INSERT：InnoDB为新插入的每一行保存当前系统版本号作为行版本号
* DELETE：InnoDB为删除的每一行保存当前系统版本号作为行删除标识
* UPDATE：InnoDB为插入的一行新纪录保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为删除标识

MVCC 只在 COMMITTED READ（读提交）和REPEATABLE READ（可重复读）两种隔离级别下工作

### MySQL对分布式事务的支持

InnoDB 提供的原生的事务支持
* 开始支持XA协议的分布式事务
* 一个分布式事务会涉及多个行动，这些行动本身是事务性的。
* 所有行动都必须一起成功完成，或者一起被回滚。

#### 分布式事务架构
> 使用分布式事务涉及一个或多个资源管理器和一个事务管理器

模型中分三块：应用程序（AP）、资源管理器（RM）、事务管理器（TM）:
* 应用程序：定义了事务的边界，指定需要做哪些事务；
* 资源管理器：提供了访问事务的方法，通常一个数据库就是一个资源管理器
* 事务管理器：协调参与了全局事务中的各个事务

![image](https://user-images.githubusercontent.com/27160394/140637721-06497e48-078b-46e8-b513-ce3810565afe.png)

分布式事务采用两段式提交（two-phase commit）的方式：
* 第一阶段所有的事务节点开始准备，告诉事务管理器ready。
* 第二阶段事务管理器告诉每个节点是commit还是rollback。如果有一个节点失败，就需要全局的节点全部rollback，以此保障事务的原子性
